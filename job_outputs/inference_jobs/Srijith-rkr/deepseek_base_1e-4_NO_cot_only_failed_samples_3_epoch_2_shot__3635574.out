Loading finetuned weights
INFO 12-03 20:57:18 llm_engine.py:237] Initializing an LLM engine (v0.6.3.post1) with config: model='Srijith-rkr/deepseek_base_1e-4_NO_cot_only_failed_samples_3_epoch', speculative_config=None, tokenizer='Srijith-rkr/deepseek_base_1e-4_NO_cot_only_failed_samples_3_epoch', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, rope_scaling=None, rope_theta=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir='/data/tir/projects/tir7/user_data/lmaben/hf_home_dir', load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=Srijith-rkr/deepseek_base_1e-4_NO_cot_only_failed_samples_3_epoch, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, mm_processor_kwargs=None)
INFO 12-03 20:57:31 model_runner.py:1056] Starting to load model Srijith-rkr/deepseek_base_1e-4_NO_cot_only_failed_samples_3_epoch...
INFO 12-03 20:57:37 weight_utils.py:243] Using model weights format ['*.safetensors']
INFO 12-03 21:24:12 model_runner.py:1067] Loading model weights took 12.8725 GB
INFO 12-03 21:24:22 gpu_executor.py:122] # GPU blocks: 3893, # CPU blocks: 546
INFO 12-03 21:24:22 gpu_executor.py:126] Maximum concurrency for 4096 tokens per request: 15.21x
INFO 12-03 21:24:25 model_runner.py:1395] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.
INFO 12-03 21:24:25 model_runner.py:1399] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.
INFO 12-03 21:24:38 model_runner.py:1523] Graph capturing finished in 13 secs.

==== Generating sample number 0 ===

WARNING 12-03 21:24:43 scheduler.py:895] Input prompt (5855 tokens) is too long and exceeds limit of 4096
WARNING 12-03 21:24:49 scheduler.py:1483] Sequence group 49 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1
WARNING 12-03 21:25:45 scheduler.py:895] Input prompt (4352 tokens) is too long and exceeds limit of 4096
WARNING 12-03 21:27:41 scheduler.py:895] Input prompt (7199 tokens) is too long and exceeds limit of 4096
WARNING 12-03 21:28:04 scheduler.py:1483] Sequence group 135 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=51
WARNING 12-03 21:29:38 scheduler.py:895] Input prompt (4453 tokens) is too long and exceeds limit of 4096
WARNING 12-03 21:29:41 scheduler.py:895] Input prompt (4633 tokens) is too long and exceeds limit of 4096
WARNING 12-03 21:31:29 scheduler.py:1483] Sequence group 228 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=101
WARNING 12-03 21:33:47 scheduler.py:1483] Sequence group 343 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=151
WARNING 12-03 21:34:38 scheduler.py:895] Input prompt (4779 tokens) is too long and exceeds limit of 4096
WARNING 12-03 21:36:12 scheduler.py:1483] Sequence group 406 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=201
WARNING 12-03 21:38:36 scheduler.py:895] Input prompt (5531 tokens) is too long and exceeds limit of 4096
WARNING 12-03 21:39:04 scheduler.py:1483] Sequence group 506 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=251
WARNING 12-03 21:39:33 scheduler.py:895] Input prompt (8833 tokens) is too long and exceeds limit of 4096
WARNING 12-03 21:40:34 scheduler.py:895] Input prompt (4141 tokens) is too long and exceeds limit of 4096
WARNING 12-03 21:40:34 scheduler.py:895] Input prompt (5263 tokens) is too long and exceeds limit of 4096
WARNING 12-03 21:41:33 scheduler.py:895] Input prompt (17704 tokens) is too long and exceeds limit of 4096
WARNING 12-03 21:41:35 scheduler.py:895] Input prompt (4419 tokens) is too long and exceeds limit of 4096
WARNING 12-03 21:41:41 scheduler.py:1483] Sequence group 625 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=301
WARNING 12-03 21:42:32 scheduler.py:895] Input prompt (5542 tokens) is too long and exceeds limit of 4096
WARNING 12-03 21:42:32 scheduler.py:895] Input prompt (4928 tokens) is too long and exceeds limit of 4096
WARNING 12-03 21:42:35 scheduler.py:895] Input prompt (5396 tokens) is too long and exceeds limit of 4096
WARNING 12-03 21:42:35 scheduler.py:895] Input prompt (7411 tokens) is too long and exceeds limit of 4096
WARNING 12-03 21:44:56 scheduler.py:1483] Sequence group 715 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=351
WARNING 12-03 21:45:30 scheduler.py:895] Input prompt (4488 tokens) is too long and exceeds limit of 4096
Written to /home/lmaben/cw/anlp/ANLP_A4_ECCO/inference_generations/leander/deepseek_base_1e-4_NO_cot_only_failed_samples_3_epoch_edit_deepseek_instruct_nrowsNone_tokens1024_temp0.0_fewshotex2_samples1_2024-12-03_21:47:55.jsonl
